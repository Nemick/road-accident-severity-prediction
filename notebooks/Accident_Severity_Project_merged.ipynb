{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677cf3a2",
   "metadata": {},
   "source": [
    "# Accident Severity Project \n",
    "\n",
    "This merged notebook combines the **clean, production-ready pipeline** (no leakage, ColumnTransformer pipeline, sensible defaults) with the **exploratory experiments** from original notebook (KNN, Decision Tree, bagging/boosting experiments, elbow plots, extra visualizations).  \n",
    "\n",
    "**How it's organized**:\n",
    "1. Data load & inspection\n",
    "2. Cleaning: sentinel handling, leakage decision, identifiers removal\n",
    "3. Feature engineering & cardinality reduction\n",
    "4. Preprocessing Pipeline (ColumnTransformer)\n",
    "5. Main Models: Random Forest & Logistic Regression (GridSearchCV)\n",
    "6. Experiments: KNN (elbow + bagging/boosting), DecisionTree with corrected GridSearch\n",
    "7. Evaluation, permutation importance, saving pipeline\n",
    "\n",
    "Run the notebook sequentially. If runtime is long, reduce CV folds or grid sizes in the 'Model grids' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "379a56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & global settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = '../data/merged_data.csv' # Path to the merged dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ef688c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe shape: (46707, 43)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nemne\\AppData\\Local\\Temp\\ipykernel_2044\\2722644493.py:2: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>collision_index</th>\n",
       "      <th>collision_year</th>\n",
       "      <th>collision_reference</th>\n",
       "      <th>location_easting_osgr</th>\n",
       "      <th>location_northing_osgr</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>police_force</th>\n",
       "      <th>legacy_collision_severity</th>\n",
       "      <th>...</th>\n",
       "      <th>urban_or_rural_area</th>\n",
       "      <th>did_police_officer_attend_scene_of_collision</th>\n",
       "      <th>trunk_road_flag</th>\n",
       "      <th>lsoa_of_collision_location</th>\n",
       "      <th>enhanced_severity_collision</th>\n",
       "      <th>vehicle_count</th>\n",
       "      <th>avg_vehicle_age</th>\n",
       "      <th>heavy_vehicle_count</th>\n",
       "      <th>casualty_count</th>\n",
       "      <th>avg_casualty_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unvalidated</td>\n",
       "      <td>2024010486807</td>\n",
       "      <td>2024</td>\n",
       "      <td>10486807</td>\n",
       "      <td>527188.0</td>\n",
       "      <td>184782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unvalidated</td>\n",
       "      <td>2024010486821</td>\n",
       "      <td>2024</td>\n",
       "      <td>10486821</td>\n",
       "      <td>528936.0</td>\n",
       "      <td>194721.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unvalidated</td>\n",
       "      <td>2024010486824</td>\n",
       "      <td>2024</td>\n",
       "      <td>10486824</td>\n",
       "      <td>552699.0</td>\n",
       "      <td>185940.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unvalidated</td>\n",
       "      <td>2024010486825</td>\n",
       "      <td>2024</td>\n",
       "      <td>10486825</td>\n",
       "      <td>545623.0</td>\n",
       "      <td>177185.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unvalidated</td>\n",
       "      <td>2024010486828</td>\n",
       "      <td>2024</td>\n",
       "      <td>10486828</td>\n",
       "      <td>536554.0</td>\n",
       "      <td>178468.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        status collision_index  collision_year collision_reference  \\\n",
       "0  Unvalidated   2024010486807            2024            10486807   \n",
       "1  Unvalidated   2024010486821            2024            10486821   \n",
       "2  Unvalidated   2024010486824            2024            10486824   \n",
       "3  Unvalidated   2024010486825            2024            10486825   \n",
       "4  Unvalidated   2024010486828            2024            10486828   \n",
       "\n",
       "   location_easting_osgr  location_northing_osgr  longitude  latitude  \\\n",
       "0               527188.0                184782.0        NaN       NaN   \n",
       "1               528936.0                194721.0        NaN       NaN   \n",
       "2               552699.0                185940.0        NaN       NaN   \n",
       "3               545623.0                177185.0        NaN       NaN   \n",
       "4               536554.0                178468.0        NaN       NaN   \n",
       "\n",
       "   police_force  legacy_collision_severity  ...  urban_or_rural_area  \\\n",
       "0             1                          3  ...                   -1   \n",
       "1             1                          3  ...                   -1   \n",
       "2             1                          3  ...                   -1   \n",
       "3             1                          3  ...                   -1   \n",
       "4             1                          3  ...                   -1   \n",
       "\n",
       "   did_police_officer_attend_scene_of_collision trunk_road_flag  \\\n",
       "0                                             3              -1   \n",
       "1                                             3              -1   \n",
       "2                                             1              -1   \n",
       "3                                             1              -1   \n",
       "4                                             1              -1   \n",
       "\n",
       "   lsoa_of_collision_location enhanced_severity_collision  vehicle_count  \\\n",
       "0                          -1                          -1            2.0   \n",
       "1                          -1                          -1            3.0   \n",
       "2                          -1                          -1            2.0   \n",
       "3                          -1                          -1            2.0   \n",
       "4                          -1                          -1            1.0   \n",
       "\n",
       "  avg_vehicle_age heavy_vehicle_count  casualty_count  avg_casualty_age  \n",
       "0            -1.0                 0.0             0.0               NaN  \n",
       "1            -1.0                 0.0             0.0               NaN  \n",
       "2            -1.0                 0.0             0.0               NaN  \n",
       "3            -1.0                 0.0             0.0               NaN  \n",
       "4            -1.0                 0.0             0.0               NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load merged data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f4a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46707 entries, 0 to 46706\n",
      "Data columns (total 43 columns):\n",
      " #   Column                                        Non-Null Count  Dtype  \n",
      "---  ------                                        --------------  -----  \n",
      " 0   status                                        46707 non-null  object \n",
      " 1   collision_index                               46707 non-null  object \n",
      " 2   collision_year                                46707 non-null  int64  \n",
      " 3   collision_reference                           46707 non-null  object \n",
      " 4   location_easting_osgr                         46623 non-null  float64\n",
      " 5   location_northing_osgr                        46623 non-null  float64\n",
      " 6   longitude                                     0 non-null      float64\n",
      " 7   latitude                                      0 non-null      float64\n",
      " 8   police_force                                  46707 non-null  int64  \n",
      " 9   legacy_collision_severity                     46707 non-null  int64  \n",
      " 10  number_of_vehicles                            46707 non-null  int64  \n",
      " 11  number_of_casualties                          46707 non-null  int64  \n",
      " 12  date                                          46707 non-null  object \n",
      " 13  day_of_week                                   46707 non-null  int64  \n",
      " 14  time                                          46707 non-null  object \n",
      " 15  local_authority_district                      46707 non-null  int64  \n",
      " 16  local_authority_ons_district                  46707 non-null  object \n",
      " 17  local_authority_highway                       46707 non-null  object \n",
      " 18  first_road_class                              46707 non-null  int64  \n",
      " 19  first_road_number                             46707 non-null  int64  \n",
      " 20  road_type                                     46707 non-null  int64  \n",
      " 21  speed_limit                                   46707 non-null  int64  \n",
      " 22  junction_detail                               46707 non-null  int64  \n",
      " 23  junction_control                              46707 non-null  int64  \n",
      " 24  second_road_class                             46707 non-null  int64  \n",
      " 25  second_road_number                            46707 non-null  int64  \n",
      " 26  pedestrian_crossing_human_control             46707 non-null  int64  \n",
      " 27  pedestrian_crossing_physical_facilities       46707 non-null  int64  \n",
      " 28  light_conditions                              46707 non-null  int64  \n",
      " 29  weather_conditions                            46707 non-null  int64  \n",
      " 30  road_surface_conditions                       46707 non-null  int64  \n",
      " 31  special_conditions_at_site                    46707 non-null  int64  \n",
      " 32  carriageway_hazards                           46707 non-null  int64  \n",
      " 33  urban_or_rural_area                           46707 non-null  int64  \n",
      " 34  did_police_officer_attend_scene_of_collision  46707 non-null  int64  \n",
      " 35  trunk_road_flag                               46707 non-null  int64  \n",
      " 36  lsoa_of_collision_location                    46707 non-null  int64  \n",
      " 37  enhanced_severity_collision                   46707 non-null  int64  \n",
      " 38  vehicle_count                                 46707 non-null  float64\n",
      " 39  avg_vehicle_age                               44839 non-null  float64\n",
      " 40  heavy_vehicle_count                           46707 non-null  float64\n",
      " 41  casualty_count                                46707 non-null  float64\n",
      " 42  avg_casualty_age                              30323 non-null  float64\n",
      "dtypes: float64(9), int64(27), object(7)\n",
      "memory usage: 15.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "longitude                                       46707\n",
       "latitude                                        46707\n",
       "avg_casualty_age                                16384\n",
       "avg_vehicle_age                                  1868\n",
       "location_northing_osgr                             84\n",
       "location_easting_osgr                              84\n",
       "collision_year                                      0\n",
       "collision_reference                                 0\n",
       "police_force                                        0\n",
       "status                                              0\n",
       "collision_index                                     0\n",
       "number_of_vehicles                                  0\n",
       "legacy_collision_severity                           0\n",
       "number_of_casualties                                0\n",
       "date                                                0\n",
       "local_authority_district                            0\n",
       "local_authority_ons_district                        0\n",
       "day_of_week                                         0\n",
       "time                                                0\n",
       "first_road_number                                   0\n",
       "road_type                                           0\n",
       "speed_limit                                         0\n",
       "junction_detail                                     0\n",
       "junction_control                                    0\n",
       "second_road_class                                   0\n",
       "local_authority_highway                             0\n",
       "first_road_class                                    0\n",
       "pedestrian_crossing_human_control                   0\n",
       "second_road_number                                  0\n",
       "pedestrian_crossing_physical_facilities             0\n",
       "light_conditions                                    0\n",
       "special_conditions_at_site                          0\n",
       "carriageway_hazards                                 0\n",
       "weather_conditions                                  0\n",
       "road_surface_conditions                             0\n",
       "did_police_officer_attend_scene_of_collision        0\n",
       "urban_or_rural_area                                 0\n",
       "trunk_road_flag                                     0\n",
       "lsoa_of_collision_location                          0\n",
       "vehicle_count                                       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick inspection\n",
    "display(df.info())\n",
    "display(df.isnull().sum().sort_values(ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e21289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns where -1 replaced with NaN (col, count): [('local_authority_district', 46707), ('first_road_number', 1), ('speed_limit', 35), ('junction_detail', 1331), ('junction_control', 19539), ('second_road_class', 501), ('second_road_number', 19143), ('pedestrian_crossing_human_control', 8950), ('pedestrian_crossing_physical_facilities', 8952), ('road_surface_conditions', 370), ('special_conditions_at_site', 8971), ('carriageway_hazards', 8969), ('urban_or_rural_area', 46707), ('trunk_road_flag', 46707), ('lsoa_of_collision_location', 46707), ('enhanced_severity_collision', 18964), ('avg_vehicle_age', 44839), ('avg_casualty_age', 383)]\n"
     ]
    }
   ],
   "source": [
    "# Convert -1 sentinel to NaN for numeric columns where appropriate\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_replaced = []\n",
    "for c in num_cols:\n",
    "    if (df[c] == -1).sum() > 0 and df[c].min() == -1:\n",
    "        cnt = int((df[c] == -1).sum())\n",
    "        df[c] = df[c].replace(-1, np.nan)\n",
    "        cols_replaced.append((c, cnt))\n",
    "print('Columns where -1 replaced with NaN (col, count):', cols_replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "625c7a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected possible casualty-derived columns: ['casualty_count', 'avg_casualty_age']\n",
      "Shape after dropping casualty-derived columns: (46707, 41)\n"
     ]
    }
   ],
   "source": [
    "# Decide on leakage: By default we drop casualty-derived features for a pre-event model.\n",
    "leakage_keywords = ['casualty', 'avg_casualty', 'casualty_count']\n",
    "leak_cols = [c for c in df.columns if any(k in c.lower() for k in leakage_keywords)]\n",
    "print('Detected possible casualty-derived columns:', leak_cols)\n",
    "# Drop them by default -- if you want retrospective model, comment out the next line\n",
    "df = df.drop(columns=[c for c in leak_cols if c in df.columns], errors='ignore')\n",
    "print('Shape after dropping casualty-derived columns:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fce49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping ids/empty geos: (46707, 36)\n"
     ]
    }
   ],
   "source": [
    "# Drop identifiers and fully-empty geolocation columns\n",
    "ids_to_drop = ['collision_index','collision_reference','status']\n",
    "for c in ids_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "# Drop lat/lon if empty or near-empty\n",
    "if 'latitude' in df.columns and df['latitude'].isna().all():\n",
    "    df = df.drop(columns=['latitude'], errors='ignore')\n",
    "if 'longitude' in df.columns and df['longitude'].isna().all():\n",
    "    df = df.drop(columns=['longitude'], errors='ignore')\n",
    "print('Shape after dropping ids/empty geos:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe1e00a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporal features: hour_of_day, day_of_week, month\n",
      "Reduced high-cardinality columns: ['local_authority_ons_district', 'local_authority_highway']\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering: datetime features\n",
    "if 'date' in df.columns and 'time' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str), errors='coerce')\n",
    "    df['hour_of_day'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df = df.drop(columns=['date','time','datetime'])\n",
    "    print('Created temporal features: hour_of_day, day_of_week, month')\n",
    "\n",
    "# Reduce cardinality for very high-cardinality object columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "high_card = [c for c in cat_cols if df[c].nunique() > 50]\n",
    "TOP_N = 50\n",
    "for c in high_card:\n",
    "    top_vals = df[c].value_counts().nlargest(TOP_N).index\n",
    "    df[c] = df[c].where(df[c].isin(top_vals), other='OTHER')\n",
    "print('Reduced high-cardinality columns:', high_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78c79dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "legacy_collision_severity\n",
       "3    0.751707\n",
       "2    0.233627\n",
       "1    0.014666\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test shapes: (37365, 35) (9342, 35)\n"
     ]
    }
   ],
   "source": [
    "# Prepare target & features\n",
    "TARGET = 'legacy_collision_severity'\n",
    "assert TARGET in df.columns, f\"Target column {TARGET} not found.\"\n",
    "\n",
    "df = df[~df[TARGET].isna()].copy()\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype('int')\n",
    "\n",
    "print('Target distribution:')\n",
    "display(y.value_counts(normalize=True))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "print('Train/test shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abc8a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: 32 Categorical cols: 3\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline with ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric cols:', len(numeric_cols), 'Categorical cols:', len(categorical_cols))\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "], n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f8507d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting RandomForest GridSearch (this may take a while)...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best RF params: {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__n_estimators': 100}\n",
      "Best RF CV score: 0.8624100020973753\n",
      "Fitting Logistic Regression GridSearch...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best LR params: {'clf__C': 0.1}\n",
      "Best LR CV score: 0.6637070179022316\n"
     ]
    }
   ],
   "source": [
    "# Main models: RandomForest and LogisticRegression (GridSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "rf_pipe = Pipeline([('preproc', preprocessor),\n",
    "                    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1))])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [10, None],\n",
    "    'clf__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "rf_gs = GridSearchCV(rf_pipe, rf_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "print('Fitting RandomForest GridSearch (this may take a while)...')\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print('Best RF params:', rf_gs.best_params_)\n",
    "print('Best RF CV score:', rf_gs.best_score_)\n",
    "\n",
    "lr_pipe = Pipeline([('preproc', preprocessor),\n",
    "                    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "lr_param_grid = {'clf__C': [0.1, 1.0, 10.0]}\n",
    "lr_gs = GridSearchCV(lr_pipe, lr_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "print('Fitting Logistic Regression GridSearch...')\n",
    "lr_gs.fit(X_train, y_train)\n",
    "print('Best LR params:', lr_gs.best_params_)\n",
    "print('Best LR CV score:', lr_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f8cc3",
   "metadata": {},
   "source": [
    "## Experiments: KNN, K-Optimal (Elbow), Bagging/Boosting, Decision Tree GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36ee2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rates (1-accuracy) for K=1..15: [np.float64(0.16600483542012778), np.float64(0.20200335267801528), np.float64(0.13449731590661118), np.float64(0.13899581740661204), np.float64(0.13299806553179871), np.float64(0.12399430915173049), np.float64(0.12399806102954525), np.float64(0.114998056527292), np.float64(0.12400106253179721), np.float64(0.11749905827866858), np.float64(0.12400106253179721), np.float64(0.12300006153079612), np.float64(0.12600081340711033), np.float64(0.12250106178142162), np.float64(0.13100056578317443)]\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best KNN params: {'clf__n_neighbors': 3} Best CV score: 0.5962162564442296\n",
      "Trained Bagging KNN (10 estimators)\n",
      "CV score: 0.5889541911374631\n"
     ]
    }
   ],
   "source": [
    "# KNN Experiment: Elbow method and optional Bagging/Boosting wrappers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Elbow method (error rate) - small sample for speed\n",
    "sample = X_train.sample(n=min(2000, X_train.shape[0]), random_state=RANDOM_STATE)\n",
    "y_sample = y_train.loc[sample.index]\n",
    "# Minimal preprocessing: use preprocessor to transform sample once\n",
    "X_sample_trans = preprocessor.fit_transform(sample)\n",
    "error_rates = []\n",
    "K_vals = range(1, 16)\n",
    "for k in K_vals:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_sample_trans, y_sample, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    error_rates.append(1 - scores.mean())\n",
    "print('Error rates (1-accuracy) for K=1..15:', error_rates)\n",
    "\n",
    "# Choose a small grid for KNN with bagging/boosting (optional heavy)\n",
    "knn_pipe = Pipeline([('preproc', preprocessor), ('clf', KNeighborsClassifier())])\n",
    "knn_param_grid = {'clf__n_neighbors': [3,5,7]}\n",
    "knn_gs = GridSearchCV(knn_pipe, knn_param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "print('Best KNN params:', knn_gs.best_params_, 'Best CV score:', knn_gs.best_score_)\n",
    "\n",
    "# Bagging KNN\n",
    "bag_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=knn_gs.best_params_['clf__n_neighbors']), n_estimators=10, random_state=RANDOM_STATE)\n",
    "bag_pipe = Pipeline([('preproc', preprocessor), ('clf', bag_knn)])\n",
    "bag_pipe.fit(X_train, y_train)\n",
    "print('Trained Bagging KNN (10 estimators)')\n",
    "print('CV score:', cross_val_score(bag_pipe, X_train, y_train, cv=3, scoring='f1_macro', n_jobs=-1).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aecd8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best DT params: {'clf__criterion': 'gini', 'clf__max_depth': 3, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2} Best score: 0.8688383672584731\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with corrected GridSearch (valid parameters)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_pipe = Pipeline([('preproc', preprocessor), ('clf', DecisionTreeClassifier(random_state=RANDOM_STATE))])\n",
    "\n",
    "dt_param_grid = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [3, 5, 10, None],\n",
    "    'clf__min_samples_leaf': [1, 2, 5],\n",
    "    'clf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "dt_gs = GridSearchCV(dt_pipe, dt_param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "dt_gs.fit(X_train, y_train)\n",
    "print('Best DT params:', dt_gs.best_params_, 'Best score:', dt_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60682190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RandomForest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.6423    0.7822       137\n",
      "           2     0.9750    0.7142    0.8244      2183\n",
      "           3     0.9122    0.9944    0.9516      7022\n",
      "\n",
      "    accuracy                         0.9238      9342\n",
      "   macro avg     0.9624    0.7836    0.8527      9342\n",
      "weighted avg     0.9282    0.9238    0.9194      9342\n",
      "\n",
      "Confusion matrix:\n",
      "[[  88    1   48]\n",
      " [   0 1559  624]\n",
      " [   0   39 6983]]\n",
      "Macro F1: 0.8527368658687259\n",
      "\n",
      "\n",
      "--- LogisticRegression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.1702    0.8102    0.2814       137\n",
      "           2     0.7949    0.7778    0.7863      2183\n",
      "           3     0.9399    0.8772    0.9075      7022\n",
      "\n",
      "    accuracy                         0.8530      9342\n",
      "   macro avg     0.6350    0.8218    0.6584      9342\n",
      "weighted avg     0.8947    0.8530    0.8700      9342\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 111    4   22]\n",
      " [ 113 1698  372]\n",
      " [ 428  434 6160]]\n",
      "Macro F1: 0.6583819132110684\n",
      "\n",
      "\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.2459    0.1095    0.1515       137\n",
      "           2     0.8374    0.6770    0.7487      2183\n",
      "           3     0.8933    0.9561    0.9236      7022\n",
      "\n",
      "    accuracy                         0.8785      9342\n",
      "   macro avg     0.6589    0.5809    0.6080      9342\n",
      "weighted avg     0.8707    0.8785    0.8715      9342\n",
      "\n",
      "Confusion matrix:\n",
      "[[  15    9  113]\n",
      " [  16 1478  689]\n",
      " [  30  278 6714]]\n",
      "Macro F1: 0.6079656857574157\n",
      "\n",
      "\n",
      "--- DecisionTree ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     1.0000    0.7153    0.8340       137\n",
      "           2     1.0000    0.7073    0.8285      2183\n",
      "           3     0.9119    1.0000    0.9539      7022\n",
      "\n",
      "    accuracy                         0.9274      9342\n",
      "   macro avg     0.9706    0.8075    0.8722      9342\n",
      "weighted avg     0.9338    0.9274    0.9229      9342\n",
      "\n",
      "Confusion matrix:\n",
      "[[  98    0   39]\n",
      " [   0 1544  639]\n",
      " [   0    0 7022]]\n",
      "Macro F1: 0.8721791527427373\n",
      "\n",
      "\n",
      "--- BaggingKNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3143    0.0803    0.1279       137\n",
      "           2     0.8341    0.6748    0.7460      2183\n",
      "           3     0.8917    0.9576    0.9234      7022\n",
      "\n",
      "    accuracy                         0.8786      9342\n",
      "   macro avg     0.6800    0.5709    0.5991      9342\n",
      "weighted avg     0.8697    0.8786    0.8703      9342\n",
      "\n",
      "Confusion matrix:\n",
      "[[  11    9  117]\n",
      " [  10 1473  700]\n",
      " [  14  284 6724]]\n",
      "Macro F1: 0.5991182434869677\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation: RF, LR, KNN (from grids) and Decision Tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "models = {\n",
    "    'RandomForest': rf_gs.best_estimator_,\n",
    "    'LogisticRegression': lr_gs.best_estimator_,\n",
    "    'KNN': knn_gs.best_estimator_,\n",
    "    'DecisionTree': dt_gs.best_estimator_,\n",
    "    'BaggingKNN': bag_pipe\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print('---', name, '---')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Macro F1:', f1_score(y_test, y_pred, average='macro'))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "117d3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best on test set: DecisionTree with macro F1: 0.8721791527427373\n",
      "Saved merged best pipeline to model/accident_severity_pipeline_merged.joblib\n"
     ]
    }
   ],
   "source": [
    "# Choose the best model by macro F1 on test set and save pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "best_name, best_score, best_model = None, -1, None\n",
    "for name, model in models.items():\n",
    "    score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_name = name\n",
    "        best_model = model\n",
    "print('Best on test set:', best_name, 'with macro F1:', best_score)\n",
    "joblib.dump(best_model, '../model/accident_severity_pipeline_merged.joblib')\n",
    "print('Saved merged best pipeline to model/accident_severity_pipeline_merged.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06b45245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not map all feature names: Length of values (35) does not match length of index (141)\n",
      "Raw importances (first 40): [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.26935 0.      0.      0.      0.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "# Permutation importance (sampled) for the chosen best_model\n",
    "from sklearn.inspection import permutation_importance\n",
    "sample_idx = np.random.choice(range(X_test.shape[0]), size=min(2000, X_test.shape[0]), replace=False)\n",
    "X_test_sample = X_test.iloc[sample_idx]\n",
    "y_test_sample = y_test.iloc[sample_idx]\n",
    "r = permutation_importance(best_model, X_test_sample, y_test_sample, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "# Attempt to retrieve feature names\n",
    "try:\n",
    "    num_feats = numeric_cols\n",
    "    ohe = best_model.named_steps['preproc'].named_transformers_['cat'].named_steps['ohe']\n",
    "    ohe_names = ohe.get_feature_names_out(categorical_cols).tolist() if hasattr(ohe, 'get_feature_names_out') else []\n",
    "    feature_names = num_feats + ohe_names\n",
    "    imp = pd.Series(r.importances_mean, index=feature_names).sort_values(ascending=False)\n",
    "    display(imp.head(30))\n",
    "except Exception as e:\n",
    "    print('Could not map all feature names:', e)\n",
    "    print('Raw importances (first 40):', r.importances_mean[:40])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
