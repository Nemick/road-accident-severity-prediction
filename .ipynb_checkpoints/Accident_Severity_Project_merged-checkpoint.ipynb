{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677cf3a2",
   "metadata": {},
   "source": [
    "# Accident Severity Project â€” Merged Notebook\n",
    "\n",
    "This merged notebook combines the **clean, production-ready pipeline** (no leakage, ColumnTransformer pipeline, sensible defaults) with the **exploratory experiments** from your original notebook (KNN, Decision Tree, bagging/boosting experiments, elbow plots, extra visualizations).  \n",
    "\n",
    "**How it's organized**:\n",
    "1. Data load & inspection\n",
    "2. Cleaning: sentinel handling, leakage decision, identifiers removal\n",
    "3. Feature engineering & cardinality reduction\n",
    "4. Preprocessing Pipeline (ColumnTransformer)\n",
    "5. Main Models: Random Forest & Logistic Regression (GridSearchCV)\n",
    "6. Experiments: KNN (elbow + bagging/boosting), DecisionTree with corrected GridSearch\n",
    "7. Evaluation, permutation importance, saving pipeline\n",
    "\n",
    "Run the notebook sequentially. If runtime is long, reduce CV folds or grid sizes in the 'Model grids' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & global settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = 'merged_data.csv'  # change if your merged CSV is elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef688c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection\n",
    "display(df.info())\n",
    "display(df.isnull().sum().sort_values(ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert -1 sentinel to NaN for numeric columns where appropriate\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_replaced = []\n",
    "for c in num_cols:\n",
    "    if (df[c] == -1).sum() > 0 and df[c].min() == -1:\n",
    "        cnt = int((df[c] == -1).sum())\n",
    "        df[c] = df[c].replace(-1, np.nan)\n",
    "        cols_replaced.append((c, cnt))\n",
    "print('Columns where -1 replaced with NaN (col, count):', cols_replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on leakage: By default we drop casualty-derived features for a pre-event model.\n",
    "leakage_keywords = ['casualty', 'avg_casualty', 'casualty_count']\n",
    "leak_cols = [c for c in df.columns if any(k in c.lower() for k in leakage_keywords)]\n",
    "print('Detected possible casualty-derived columns:', leak_cols)\n",
    "# Drop them by default -- if you want retrospective model, comment out the next line\n",
    "df = df.drop(columns=[c for c in leak_cols if c in df.columns], errors='ignore')\n",
    "print('Shape after dropping casualty-derived columns:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifiers and fully-empty geolocation columns\n",
    "ids_to_drop = ['collision_index','collision_reference','status']\n",
    "for c in ids_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "# Drop lat/lon if empty or near-empty\n",
    "if 'latitude' in df.columns and df['latitude'].isna().all():\n",
    "    df = df.drop(columns=['latitude'], errors='ignore')\n",
    "if 'longitude' in df.columns and df['longitude'].isna().all():\n",
    "    df = df.drop(columns=['longitude'], errors='ignore')\n",
    "print('Shape after dropping ids/empty geos:', df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: datetime features\n",
    "if 'date' in df.columns and 'time' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str), errors='coerce')\n",
    "    df['hour_of_day'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df = df.drop(columns=['date','time','datetime'])\n",
    "    print('Created temporal features: hour_of_day, day_of_week, month')\n",
    "\n",
    "# Reduce cardinality for very high-cardinality object columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "high_card = [c for c in cat_cols if df[c].nunique() > 50]\n",
    "TOP_N = 50\n",
    "for c in high_card:\n",
    "    top_vals = df[c].value_counts().nlargest(TOP_N).index\n",
    "    df[c] = df[c].where(df[c].isin(top_vals), other='OTHER')\n",
    "print('Reduced high-cardinality columns:', high_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c79dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target & features\n",
    "TARGET = 'legacy_collision_severity'\n",
    "assert TARGET in df.columns, f\"Target column {TARGET} not found.\"\n",
    "\n",
    "df = df[~df[TARGET].isna()].copy()\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype('int')\n",
    "\n",
    "print('Target distribution:')\n",
    "display(y.value_counts(normalize=True))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "print('Train/test shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print('Numeric cols:', len(numeric_cols), 'Categorical cols:', len(categorical_cols))\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=True))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "], n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main models: RandomForest and LogisticRegression (GridSearchCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "rf_pipe = Pipeline([('preproc', preprocessor),\n",
    "                    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1))])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [10, None],\n",
    "    'clf__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "rf_gs = GridSearchCV(rf_pipe, rf_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "print('Fitting RandomForest GridSearch (this may take a while)...')\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print('Best RF params:', rf_gs.best_params_)\n",
    "print('Best RF CV score:', rf_gs.best_score_)\n",
    "\n",
    "lr_pipe = Pipeline([('preproc', preprocessor),\n",
    "                    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "lr_param_grid = {'clf__C': [0.1, 1.0, 10.0]}\n",
    "lr_gs = GridSearchCV(lr_pipe, lr_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "print('Fitting Logistic Regression GridSearch...')\n",
    "lr_gs.fit(X_train, y_train)\n",
    "print('Best LR params:', lr_gs.best_params_)\n",
    "print('Best LR CV score:', lr_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f8cc3",
   "metadata": {},
   "source": [
    "## Experiments: KNN, K-Optimal (Elbow), Bagging/Boosting, Decision Tree GridSearch (corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Experiment: Elbow method and optional Bagging/Boosting wrappers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Elbow method (error rate) - small sample for speed\n",
    "sample = X_train.sample(n=min(2000, X_train.shape[0]), random_state=RANDOM_STATE)\n",
    "y_sample = y_train.loc[sample.index]\n",
    "# Minimal preprocessing: use preprocessor to transform sample once\n",
    "X_sample_trans = preprocessor.fit_transform(sample)\n",
    "error_rates = []\n",
    "K_vals = range(1, 16)\n",
    "for k in K_vals:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_sample_trans, y_sample, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    error_rates.append(1 - scores.mean())\n",
    "print('Error rates (1-accuracy) for K=1..15:', error_rates)\n",
    "\n",
    "# Choose a small grid for KNN with bagging/boosting (optional heavy)\n",
    "knn_pipe = Pipeline([('preproc', preprocessor), ('clf', KNeighborsClassifier())])\n",
    "knn_param_grid = {'clf__n_neighbors': [3,5,7]}\n",
    "knn_gs = GridSearchCV(knn_pipe, knn_param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "print('Best KNN params:', knn_gs.best_params_, 'Best CV score:', knn_gs.best_score_)\n",
    "\n",
    "# Bagging KNN\n",
    "bag_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=knn_gs.best_params_['clf__n_neighbors']), n_estimators=10, random_state=RANDOM_STATE)\n",
    "bag_pipe = Pipeline([('preproc', preprocessor), ('clf', bag_knn)])\n",
    "bag_pipe.fit(X_train, y_train)\n",
    "print('Trained Bagging KNN (10 estimators)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with corrected GridSearch (valid parameters)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_pipe = Pipeline([('preproc', preprocessor), ('clf', DecisionTreeClassifier(random_state=RANDOM_STATE))])\n",
    "\n",
    "dt_param_grid = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [3, 5, 10, None],\n",
    "    'clf__min_samples_leaf': [1, 2, 5],\n",
    "    'clf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "dt_gs = GridSearchCV(dt_pipe, dt_param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "dt_gs.fit(X_train, y_train)\n",
    "print('Best DT params:', dt_gs.best_params_, 'Best score:', dt_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60682190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation: RF, LR, KNN (from grids) and Decision Tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "models = {\n",
    "    'RandomForest': rf_gs.best_estimator_,\n",
    "    'LogisticRegression': lr_gs.best_estimator_,\n",
    "    'KNN': knn_gs.best_estimator_,\n",
    "    'DecisionTree': dt_gs.best_estimator_,\n",
    "    'BaggingKNN': bag_pipe\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print('---', name, '---')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Macro F1:', f1_score(y_test, y_pred, average='macro'))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model by macro F1 on test set and save pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "best_name, best_score, best_model = None, -1, None\n",
    "for name, model in models.items():\n",
    "    score = f1_score(y_test, model.predict(X_test), average='macro')\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_name = name\n",
    "        best_model = model\n",
    "print('Best on test set:', best_name, 'with macro F1:', best_score)\n",
    "joblib.dump(best_model, '/mnt/data/accident_severity_pipeline_merged.joblib')\n",
    "print('Saved merged best pipeline to /mnt/data/accident_severity_pipeline_merged.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b45245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance (sampled) for the chosen best_model\n",
    "from sklearn.inspection import permutation_importance\n",
    "sample_idx = np.random.choice(range(X_test.shape[0]), size=min(2000, X_test.shape[0]), replace=False)\n",
    "X_test_sample = X_test.iloc[sample_idx]\n",
    "y_test_sample = y_test.iloc[sample_idx]\n",
    "r = permutation_importance(best_model, X_test_sample, y_test_sample, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "# Attempt to retrieve feature names\n",
    "try:\n",
    "    num_feats = numeric_cols\n",
    "    ohe = best_model.named_steps['preproc'].named_transformers_['cat'].named_steps['ohe']\n",
    "    ohe_names = ohe.get_feature_names_out(categorical_cols).tolist() if hasattr(ohe, 'get_feature_names_out') else []\n",
    "    feature_names = num_feats + ohe_names\n",
    "    imp = pd.Series(r.importances_mean, index=feature_names).sort_values(ascending=False)\n",
    "    display(imp.head(30))\n",
    "except Exception as e:\n",
    "    print('Could not map all feature names:', e)\n",
    "    print('Raw importances (first 40):', r.importances_mean[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986fd0e",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "This merged notebook retains best practices (pipelines, no leakage) while including all exploratory experiments. Next steps:\n",
    "- Run SHAP on the best tree-based model for deeper explainability.\n",
    "- Consider balanced-batch training or SMOTE for rare classes if you want to improve recall for the minority class.\n",
    "- Prepare a small Flask/FastAPI wrapper to serve the saved pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
