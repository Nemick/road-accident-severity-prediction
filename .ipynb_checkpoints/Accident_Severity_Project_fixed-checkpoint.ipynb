{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa547dda",
   "metadata": {},
   "source": [
    "\n",
    "# Accident Severity Project — Fixed & Improved Notebook\n",
    "\n",
    "This notebook applies the recommended improvements:\n",
    "\n",
    "- Convert `-1` sentinel values to `NaN` where appropriate.\n",
    "- Drop leakage features (casualty-derived) by default so the model predicts pre-event severity.\n",
    "- Use `ColumnTransformer` + `Pipeline` to avoid data leakage during cross-validation.\n",
    "- Reduce cardinality for high-cardinality categorical features.\n",
    "- Use `StratifiedKFold` and `GridSearchCV` with conservative parameter grids for speed.\n",
    "- Save the full pipeline (preprocessing + model) with `joblib`.\n",
    "\n",
    "**Note:** the notebook is intentionally conservative on grid sizes and CV folds to keep runtime reasonable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "sns.set(style='darkgrid')\n",
    "DATA_PATH = '/mnt/data/merged_data.csv'\n",
    "\n",
    "# Load merged dataset produced earlier\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded dataset shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cabb37",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Inspect dataset\n",
    "We'll look at `info()` and missing value counts to see where `-1` or nulls appear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df.info())\n",
    "display(df.isnull().sum().sort_values(ascending=False).head(40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affded10",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Convert `-1` sentinel values to `NaN` where appropriate\n",
    "Many of these datasets use `-1` to denote *unknown/missing*. We'll convert `-1` → `np.nan` only for numeric columns where `-1` appears and likely indicates missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace -1 with NaN for numeric columns where -1 is used as sentinel\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_replaced = []\n",
    "for c in num_cols:\n",
    "    cnt_minus_one = int((df[c] == -1).sum())\n",
    "    if cnt_minus_one > 0 and df[c].min() == -1:\n",
    "        df[c] = df[c].replace(-1, np.nan)\n",
    "        cols_replaced.append((c, cnt_minus_one))\n",
    "print('Replaced -1 -> NaN in columns (col, count -1):')\n",
    "print(cols_replaced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174f2a0",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Leakage decision: drop casualty-derived columns (default)\n",
    "By default this notebook builds a **pre-event** prediction model (predict severity *before* casualty counts are known). Therefore casualty-derived columns such as `casualty_count` and `avg_casualty_age` will be dropped to avoid leakage. If you prefer a retrospective model (using casualty info), you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify likely casualty-derived columns and drop them to avoid leakage for pre-event modelling\n",
    "possible_leakage = [c for c in df.columns if 'casualty' in c.lower() or 'casualty_count' in c.lower() or 'avg_casualty' in c.lower()]\n",
    "# Common ones in your merged file\n",
    "default_drop = ['casualty_count', 'avg_casualty_age']\n",
    "to_drop = [c for c in default_drop if c in df.columns]\n",
    "print('Dropping columns to avoid leakage:', to_drop)\n",
    "df = df.drop(columns=to_drop, errors='ignore')\n",
    "print('New shape after dropping leakage cols:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e8d3c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Drop obviously unusable or identifier columns\n",
    "We'll remove identifier columns and completely-empty geolocation columns (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop identifier columns & empty geolocation columns\n",
    "drop_ids = ['collision_index', 'collision_reference', 'status']\n",
    "for c in drop_ids:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "# Drop latitude/longitude if they are all-null\n",
    "if 'latitude' in df.columns and df['latitude'].isna().all():\n",
    "    df = df.drop(columns=['latitude'])\n",
    "if 'longitude' in df.columns and df['longitude'].isna().all():\n",
    "    df = df.drop(columns=['longitude'])\n",
    "print('Shape after dropping ids and empty geos:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10807d29",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Feature engineering & cardinality reduction\n",
    "- If `date`/`time` are present convert them to temporal features.\n",
    "- For categorical columns with very high cardinality, keep top N categories and replace others with `'OTHER'` to limit dummy explosion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If date/time exist, create features\n",
    "if 'date' in df.columns and 'time' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "    df['hour_of_day'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "    df = df.drop(columns=['date','time','datetime'])\n",
    "\n",
    "# Cardinality reduction: for object columns with >50 unique values, keep top 50 categories\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "high_card_cols = [c for c in cat_cols if df[c].nunique() > 50]\n",
    "TOP_N = 50\n",
    "for c in high_card_cols:\n",
    "    top_vals = df[c].value_counts().nlargest(TOP_N).index\n",
    "    df[c] = df[c].where(df[c].isin(top_vals), other='OTHER')\n",
    "print('High-cardinal columns reduced:', high_card_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62189cff",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Prepare features (X) and target (y)\n",
    "Target column: `legacy_collision_severity` (1=Fatal, 2=Serious, 3=Slight)\n",
    "We'll create a train/test split stratified by the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf24465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = 'legacy_collision_severity'\n",
    "assert TARGET in df.columns, f\"Target {TARGET} not found in dataframe\"\n",
    "# Drop rows with missing target\n",
    "df = df[~df[TARGET].isna()].copy()\n",
    "# Features and target\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "# Quick class distribution\n",
    "print('Target distribution:')\n",
    "print(y.value_counts(normalize=True))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b325b",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Build preprocessing pipeline (ColumnTransformer) and modeling pipelines\n",
    "We use `SimpleImputer` for numerical & categorical, `StandardScaler` for numerical scaling, and `OneHotEncoder` for categorical encoding (with `handle_unknown='ignore'`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Identify numeric and categorical columns after cleaning\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "], n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ff90f",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Model training with GridSearchCV (Random Forest + Logistic Regression)\n",
    "We keep parameter grids small to keep runtime reasonable. We'll use `StratifiedKFold` and `f1_macro` scoring to handle class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RandomForest pipeline + GridSearch\n",
    "rf_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [10, None],\n",
    "    'clf__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rf_gs = GridSearchCV(rf_pipe, rf_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print('Best RF params:', rf_gs.best_params_)\n",
    "print('Best RF CV score:', rf_gs.best_score_)\n",
    "\n",
    "# Logistic Regression pipeline + grid\n",
    "lr_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    'clf__C': [0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "lr_gs = GridSearchCV(lr_pipe, lr_param_grid, cv=cv, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "print('Best LR params:', lr_gs.best_params_)\n",
    "print('Best LR CV score:', lr_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba048cd",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Final evaluation on hold-out test set\n",
    "We'll evaluate both RF and LR on the test set with classification reports and confusion matrices. We'll also save the best-performing pipeline as a joblib file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# RF evaluation\n",
    "best_rf = rf_gs.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print('Random Forest Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n",
    "print('Confusion Matrix (RF):')\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# LR evaluation\n",
    "best_lr = lr_gs.best_estimator_\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "print('\\nLogistic Regression Classification Report:')\n",
    "print(classification_report(y_test, y_pred_lr, digits=4))\n",
    "print('Confusion Matrix (LR):')\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# Choose best by macro F1 on test set\n",
    "from sklearn.metrics import f1_score\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
    "print('\\nTest macro F1 - RF:', f1_rf, 'LR:', f1_lr)\n",
    "\n",
    "best_pipeline = best_rf if f1_rf >= f1_lr else best_lr\n",
    "joblib.dump(best_pipeline, '/mnt/data/accident_severity_pipeline.joblib')\n",
    "print('Saved best pipeline to /mnt/data/accident_severity_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d03df",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Permutation importance (optional, on a subset to save time)\n",
    "Permutation importance gives a model-agnostic estimate of feature importance. We'll compute this on the test set (or a sample) to identify top predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b26b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "# Transform X_test through preprocessor to avoid repeating encoding in importance\n",
    "# Use the pipeline's preprocessing + classifier for permutation importance\n",
    "sample_idx = np.random.choice(range(X_test.shape[0]), size=min(2000, X_test.shape[0]), replace=False)\n",
    "X_test_sample = X_test.iloc[sample_idx]\n",
    "y_test_sample = y_test.iloc[sample_idx]\n",
    "\n",
    "r = permutation_importance(best_pipeline, X_test_sample, y_test_sample, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "# Get top 20 features\n",
    "try:\n",
    "    feature_names_num = numeric_cols\n",
    "    # when OHE used, ColumnTransformer produces many features — we'll approximate names for OHE columns\n",
    "    ohe = best_pipeline.named_steps['preproc'].named_transformers_['cat'].named_steps['ohe']\n",
    "    ohe_feature_names = ohe.get_feature_names_out(categorical_cols).tolist() if hasattr(ohe, 'get_feature_names_out') else []\n",
    "    all_feature_names = feature_names_num + ohe_feature_names\n",
    "    importances = pd.Series(r.importances_mean, index=all_feature_names).sort_values(ascending=False)\n",
    "    display(importances.head(20))\n",
    "except Exception as e:\n",
    "    print('Could not map feature names exactly:', e)\n",
    "    print('Showing raw importances (first 40):', r.importances_mean[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ba000",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Next steps & improvements\n",
    "- Consider using SHAP for deeper explainability.\n",
    "- If you want a retrospective model (use casualty-derived features), re-run with those columns included.\n",
    "- Try reducing the dataset to a balanced subset for rare classes (or use SMOTE inside a pipeline with caution).\n",
    "- For deployment, wrap the saved pipeline in a small Flask/FastAPI app and serve with the same preprocessing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
